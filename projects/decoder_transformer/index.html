<!doctype html><html lang=en><head><meta charset=UTF-8><link rel=stylesheet href=/css/style.css><title>Chatbot from Scratch @ Odin Hoff Gardå</title><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css integrity=sha384-5TcZemv2l/9On385z///+d7MSYlvIEw9FuZTIdZ14vJLqWphw7e7ZPuOiCHJcFCP crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js integrity=sha384-cMkvdD8LoxVzGF/RPUKAcvmm49FQ0oxwDF3BGKtDXcEc+T1b2N+teh/OJfpU0jr6 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></head><body><div class=wrapper><header><div class=header-text-outer><h1 class=header-text>Odin Hoff Gardå</h1><div class=disk-container><div class=disk></div><div class=disk></div><div class=disk></div></div></div><nav class=nav-menu><a href=/ class=nav-link>Home</a> <a href=/research/ class=nav-link>Research</a> <a href=/projects/ class=nav-link>Projects</a> <a href=/contact/ class=nav-link>Contact</a> <a href=/vilma/ class=nav-link>Vilma</a> <a href=/misc/ class=nav-link>Misc.</a></nav></header><main><div class=back-button><a href=/projects class="btn btn-primary">&larr; Back to projects</a></div><h2 class=page-title>&#8618;&#xFE0E; Chatbot from Scratch</h2><div><p>For the course &ldquo;Deep Learning (INF265)&rdquo; at the University of Bergen (UiB) in the spring of 2025, I created a project where the goal was to implement a transformer model from scratch and train it on a dataset of question-answer pairs. The model is a decoder-only transformer that uses causal self-attention to generate answers to short questions. This is similar to the GPT-2 architecture, but much smaller and simpler.</p><p>I think this is a good exercise to understand the inner workings of transformer models and how they can be used for text generation tasks. And for fun, I also created a simple chatbot interface using Streamlit to interact with the trained model.</p><p>The repository contains the full implementation and a notebook for training on Google Colab in only a few hours.</p></div><div><a href=https://github.com/odinhg/decoder-answer-bot><img src="https://img.shields.io/badge/GitHub-Repo-181717.svg?logo=github&style=flat-square" alt="GitHub Repo"></a></div></main></div><footer>© 2025 Built by me with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> and <a href=https://neovim.io/ target=_blank rel=noopener>Neovim</a>.</footer></body></html>